{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = pd.DataFrame({\n",
    "    'bud_names': [\n",
    "        'Credit Card Payment',\n",
    "        'Utility Bill',\n",
    "        'Salary',\n",
    "        'Online Purchase',\n",
    "        'Grocery Expenses'\n",
    "    ],\n",
    "    'bud_desc': [\n",
    "        'A payment made using a credit card for a purchase.',\n",
    "        'A monthly utility bill payment for electricity and water.',\n",
    "        'Income received as salary for work done.',\n",
    "        'A payment for an online purchase from a retail website.',\n",
    "        'Expenses related to groceries and daily essentials.'\n",
    "    ],\n",
    "    'ent_names': [\n",
    "        'Expense via Credit Card',\n",
    "        'Monthly Services Payment',\n",
    "        'Monthly Earnings',\n",
    "        'Online Shopping Expenses',\n",
    "        'Grocery Store Costs'\n",
    "    ],\n",
    "    'ent_desc': [\n",
    "        'A payment using a credit card for an expense.',\n",
    "        'A monthly payment for various services.',\n",
    "        'Earnings received on a monthly basis.',\n",
    "        'Expenses related to online shopping activities.',\n",
    "        'Costs associated with a local grocery store.'\n",
    "    ]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'category_data.xlsx'\n",
    "labeled_data = pd.read_excel(path, sheet_name='labeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data['ent_desc'] = labeled_data['ent_desc'].apply(preprocess_text)\n",
    "labeled_data['bud_desc'] = labeled_data['bud_desc'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    labeled_data['ent_desc'],\n",
    "    labeled_data['bud_desc'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "\n",
    "dimensions = X_train_vect.shape[1]\n",
    "classes = len(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def create_model(hidden_layers=2, units=128, learning_rate=0.001):\n",
    "    model = keras.Sequential([\n",
    "        Input(shape=(dimensions,)),\n",
    "    ])\n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'hidden_layers': [1, 2, 3],\n",
    "    'units': [64, 128, 256],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "model = keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=hyperparams, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train_vect, y_train_encoded)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = best_model.evaluate(X_test_vect, y_test_encoded)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
